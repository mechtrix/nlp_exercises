[
  {
    "objectID": "tf_idf.html",
    "href": "tf_idf.html",
    "title": "Wörter und Dokument Häufigkeit",
    "section": "",
    "text": "Eine der Hauptfragen in NLP ist, zu quantifizieren um was es in dem Dokument geht unter Benutzung der Wörter die enthalten sind. Während wir diese “Term Häufigkeit” (“term frequency” - \\(tf\\)) messen können, können wir ebenfalls die “inverse Dokumentenhäufigkeit” (“inverse document frequency” - \\(idf\\)) berechnen. Die Term Häufigkeit ist relativ einfach zu verstehen:\n\nWie oft kommt ein Wort vor?\n\nDie inverse Dokumentenhäufigkeit versucht, Wörter nach der Wichtigkeit zu gewichten.\n\nWie wichtig ist Wort?\n\nDies wird auch durch einfaches Zählen bestimmt.\n\n\nEine Sammlung von Texten sei eine Dokumenten Sammlung. Jeder Text darin ist ein Dokument:\n\nDokument 1: “Die Katze schläft auf dem Sofa.”\nDokument 2: “Der Hund läuft im Garten.”\nDokument 3: “Die Katze jagd die Maus.”\n\nEs soll nun die “Wichtigkeit” des Wortes Katze bestimmt werden.\n\n\n\\[ IDF(wort) = \\log \\left( \\frac{\\text{Anzahl aller Dokumente}}{\\text{Anzahl aller Dokumente, die das Wort enthalten}} \\right)\\]\nDie Anzahl aller Dokumente (Sätze) beträgt: \\(n_{Dokumente} = 3\\).\nDie Anzahl aller Dokumente, die das Wort “Katze” enthalten: \\(n_{\\text{enthalten Wort}} = 2\\)\n\\[IDF(Katze)\\approx 0.18\\]\nJe kleiner der \\(IDF\\), desto unwichtiger ist das Wort. Das Wort ist weniger speziell.\nJe größer der \\(IDF\\), desto wichtiger ist das Wort für die Dokumente.",
    "crumbs": [
      "Wörter und Dokument Häufigkeit"
    ]
  },
  {
    "objectID": "tf_idf.html#beispiel-idf",
    "href": "tf_idf.html#beispiel-idf",
    "title": "Wörter und Dokument Häufigkeit",
    "section": "",
    "text": "Eine Sammlung von Texten sei eine Dokumenten Sammlung. Jeder Text darin ist ein Dokument:\n\nDokument 1: “Die Katze schläft auf dem Sofa.”\nDokument 2: “Der Hund läuft im Garten.”\nDokument 3: “Die Katze jagd die Maus.”\n\nEs soll nun die “Wichtigkeit” des Wortes Katze bestimmt werden.\n\n\n\\[ IDF(wort) = \\log \\left( \\frac{\\text{Anzahl aller Dokumente}}{\\text{Anzahl aller Dokumente, die das Wort enthalten}} \\right)\\]\nDie Anzahl aller Dokumente (Sätze) beträgt: \\(n_{Dokumente} = 3\\).\nDie Anzahl aller Dokumente, die das Wort “Katze” enthalten: \\(n_{\\text{enthalten Wort}} = 2\\)\n\\[IDF(Katze)\\approx 0.18\\]\nJe kleiner der \\(IDF\\), desto unwichtiger ist das Wort. Das Wort ist weniger speziell.\nJe größer der \\(IDF\\), desto wichtiger ist das Wort für die Dokumente.",
    "crumbs": [
      "Wörter und Dokument Häufigkeit"
    ]
  },
  {
    "objectID": "tf_idf.html#zipfsches-gesetz",
    "href": "tf_idf.html#zipfsches-gesetz",
    "title": "Wörter und Dokument Häufigkeit",
    "section": "Zipf’sches Gesetz",
    "text": "Zipf’sches Gesetz",
    "crumbs": [
      "Wörter und Dokument Häufigkeit"
    ]
  },
  {
    "objectID": "get_started.html",
    "href": "get_started.html",
    "title": "Anfänge in NLP",
    "section": "",
    "text": "Im ersten Schritt lesen wir die Daten ein. Hierzu benutzen wir die Statistikumgebung R, sie müssen aber nicht viel über Programmieren wissen um die Befehle auszuführen. Wir müssen die Daten leider ein wenig beschränken, da wir sonst nicht mehr alles in einer Tabelle darstellen können.",
    "crumbs": [
      "Anfänge in NLP"
    ]
  },
  {
    "objectID": "get_started.html#stop-wörter",
    "href": "get_started.html#stop-wörter",
    "title": "Anfänge in NLP",
    "section": "Stop Wörter",
    "text": "Stop Wörter\nEs gibt einige Wörter die grammatikalisch zwar nötig sind, aber nicht viel zum Satzverständnis beitragen. Diese Wörter werden stop words genannt. Im Englischen sind dies extrem häufige Wörter wie z.B. “the”, “of”, “to”. Damit nicht jeder eigene stop words definieren muss, gibt es vorbereitete Lexika, die öffentlich und frei zugänglich sind (Lewis et al. 2004). Aus unseren Daten können wir die stop words mit einem anti join entfernen. Mit dem nrow() Befehl sehen wir schnell, dass sich die Anzahl der Datensätze drastisch auf \\(\\approx 217.000\\) verringert hat.",
    "crumbs": [
      "Anfänge in NLP"
    ]
  },
  {
    "objectID": "get_started.html#top-10-der-bücher",
    "href": "get_started.html#top-10-der-bücher",
    "title": "Anfänge in NLP",
    "section": "Top 10 der Bücher",
    "text": "Top 10 der Bücher\nWir wollen noch wissen inwiefern die Bücher sich voneinander unterscheiden und analysieren die Top 10 eines jeden Buches.\n\n\n\n\n\n\n\n\nDie dazugehörige grafische Darstellung ist dann eben unten zu sehen.\n\n\n\n\n\n\n\n\nAm häufigsten in den Büchern kommen vor allem die Namen der Figuren vor! Das ist nicht sonderlich überaschend, bestätigt aber auch den Schreibstil von Jane Austen.",
    "crumbs": [
      "Anfänge in NLP"
    ]
  },
  {
    "objectID": "get_started.html#top-20-der-bücher",
    "href": "get_started.html#top-20-der-bücher",
    "title": "Anfänge in NLP",
    "section": "Top 20 der Bücher",
    "text": "Top 20 der Bücher\nWir können die Top der Wörter auch auf die Top 20 ändern.\n\n\n\n\n\n\n\n\nDie dazugehörige grafische Darstellung ist dann eben unten zu sehen.",
    "crumbs": [
      "Anfänge in NLP"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Natural Language Processing - praktische Übungen",
    "section": "",
    "text": "Hier finden Sie einfache Übungen zum Thema Natural Language Processing - ein großes Wort für eine einfache Idee: Text in seine Bestandteile zu zerlegen und quantitative nutzbar zu machen.\n\n\n\n\nJane Austen\n\n\n\nIn diesen Übungen konzentrieren wir uns im ersten Schritt auf die sehr bekannten Romane von Jane Austen:\n\nsensesensibility: Sense and Sensibility (deutsch: Verstand und Gefühl), veröffentlicht 1811\nprideprejudice: Pride and Prejudice (deutsch: Stolz und Vorurteil), veröffentlicht 1813\nmansfieldpark: Mansfield Park, veröffentlicht 1814\nemma: Emma, veröffentlicht 1815\nnorthangerabbey: Northanger Abbey (deutsch: Die Abtei von Northanger), posthum veröffentlicht 1818\npersuasion: Persuasion (auch: Anne Elliot), posthum veröffentlicht 1818\n\nWir werden uns aus zwei Gründen die Romane in englischer Sprache ansehen:\n\nKI Modelle basieren zum größten Teil auf englischer Sprache - das macht es einfacher\nWir müssen die Romane selber gar nicht verstehen!\n\nVielen Dank an Project Gutenberg das digitale Kopien von über \\(70.000\\) Büchern in digitaler Form zur Verfügung stellt, sowie an Julia Silge die das package für R verfügbar gemacht hat (Silge 2022). Weiterführende Literatur ist auch hier zu finden (Silge and Robinson 2017).\n\n\n\n\nReferences\n\nSilge, Julia. 2022. Janeaustenr: Jane Austen’s Complete Novels. https://CRAN.R-project.org/package=janeaustenr.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R. Sebastopol, CA: O’Reilly Media."
  }
]