---
title: "Wörter und Dokument Häufigkeit"
format: live-html
engine: knitr
bibliography: https://api.citedrive.com/bib/1617f52c-097c-4db0-9612-9f04ceee00e1/references.bib?x=eyJpZCI6ICIxNjE3ZjUyYy0wOTdjLTRkYjAtOTYxMi05ZjA0Y2VlZTAwZTEiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICJiZThhOWMzYmFkNmNkYWYxMjU0YjYyMDMxZGRiNzY5MzkwODU5NWY3MDk1NWQ4OGEzMTc3YjM2YTA5MGM4NDgzIn0=
webr:
  packages:
      - dplyr
      - ggplot2
      - stringr
      - janeaustenr
      - tidytext
      - DT
      - wordcloud
editor_options: 
  chunk_output_type: console
---

{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}

# Textquantifizierung

Eine der Hauptfragen in NLP ist, zu quantifizieren um was es in dem Dokument geht unter Benutzung der Wörter die enthalten sind.
Während wir diese "Term Häufigkeit" ("term frequency" - $tf$) messen können, können wir ebenfalls die "inverse Dokumentenhäufigkeit" ("inverse document frequency" - $idf$) berechnen.
Die Term Häufigkeit ist relativ einfach zu verstehen: 

<center>
Wie oft kommt ein Wort vor?
</center>

Die inverse Dokumentenhäufigkeit versucht, Wörter nach der Wichtigkeit zu gewichten.

<center>
Wie wichtig ist Wort?
</center>

Dies wird auch durch einfaches Zählen bestimmt.

## Beispiel idf

Eine Sammlung von Texten sei eine *Dokumenten Sammlung*.
Jeder Text darin ist ein Dokument:

- Dokument 1: "Die Katze schläft auf dem Sofa."
- Dokument 2: "Der Hund läuft im Garten."
- Dokument 3: "Die Katze jagd die Maus."

Es soll nun die "Wichtigkeit" des Wortes *Katze* bestimmt werden.

### Formel

$$ IDF(wort) = \log \left( \frac{\text{Anzahl aller Dokumente}}{\text{Anzahl aller Dokumente, die das Wort enthalten}} \right)$$

Die Anzahl aller *Dokumente* (Sätze) beträgt: $n_{Dokumente} = 3$.

Die Anzahl aller *Dokumente*, die das Wort "Katze" enthalten: $n_{\text{enthalten Wort}} = 2$

$$IDF(Katze)\approx 0.18$$

Je kleiner der $IDF$, desto unwichtiger ist das Wort. Das Wort ist weniger speziell.

Je größer der $IDF$, desto wichtiger ist das Wort für die Dokumente.

# Termhäufigkeit

Im Nachfolgenden wird die Termhäufigkeit in allen Werken von Jane Austen berechnet

```{webr}

book_words = austen_books() %>% 
  unnest_tokens(word, text) %>% 
  count(book, word, sort = TRUE)

# count number of terms in each book
total_words = book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))
#join both
book_words = left_join(book_words, total_words)

book_words |> slice_sample(n=100)|> datatable()

```

Die Ergebnistabelle enthält ein Wort/ein Buch.
`n` bezeichnet wie oft dieses Wort in diesem Buch verwendet wird, während `total` die komplette Anzahl aller Wörter in dem speziellen Buch darstellt.
Im ersten schauen wir uns die Verteilung von `n/total` für jeden Roman an.
Wir stellen also die Häufigkeit der Terme im Bezug zur kompletten Häufigkeit der Wörter dar: die *Termhäufigkeit* (*tf*)

```{webr}
#| fig-height: 7


book_words |> 
  ggplot(
    aes(
      x = n/total,
      fill = book
    )
  )+
  geom_histogram()+
  facet_wrap(
    ~book,
    scales = "free_y"
  )+
  labs(
    fill = ""
  )+
  scale_fill_brewer(
    palette = "Dark2"
  )+
  scale_y_continuous(
    expand = c(0,0,0.05,0)
  )+
  scale_x_continuous(
    limits = c(NA, 0.0009),
    labels = scales::percent
  )+
  theme_bw()+
  theme(
    legend.position = "bottom"
    )

```

## Zipf'sches Gesetz