---
title: "Anfänge in NLP"
format: live-html
engine: knitr
bibliography: https://api.citedrive.com/bib/1617f52c-097c-4db0-9612-9f04ceee00e1/references.bib?x=eyJpZCI6ICIxNjE3ZjUyYy0wOTdjLTRkYjAtOTYxMi05ZjA0Y2VlZTAwZTEiLCAidXNlciI6ICI1NjQ5IiwgInNpZ25hdHVyZSI6ICJiZThhOWMzYmFkNmNkYWYxMjU0YjYyMDMxZGRiNzY5MzkwODU5NWY3MDk1NWQ4OGEzMTc3YjM2YTA5MGM4NDgzIn0=
webr:
  packages:
      - dplyr
      - ggplot2
      - stringr
      - janeaustenr
      - tidytext
      - DT
      - wordcloud
editor_options: 
  chunk_output_type: console
---

{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}

```{r}
#| label: setup
#| include: false

library(DT)
library(janeaustenr)
library(dplyr)

```


# Die Bücher - Rohdaten

Im ersten Schritt lesen wir die Daten ein.
Hierzu benutzen wir die Statistikumgebung `R`, sie müssen aber nicht viel über Programmieren wissen um die Befehle auszuführen.
Wir müssen die Daten leider ein wenig beschränken, da wir sonst nicht mehr alles in einer Tabelle darstellen können.

```{r}
#| echo: false

austen_books() |> head(n=100) |> datatable()

```

# Vorbereitung der Daten

Unten werden Sie zum ersten Mal selber tätig.
Vieles mag im ersten Schritt kryptisch aussehen, aber am Ende des Tages sind es recht einfache Übungen:
Mit dem Befehl `austen_books()` holen wir die Rohdaten in unsere Programmierumgebung.
Für die weitere Analyse der Daten gruppieren wir die Daten nach *Buch* (`group_by(book)`).
Der `mutate()` Befehl ist schon komplizierter, aber eigentlich bauen wir uns nur eine Variable, die sagt in welcher Zeile im Buch der Text steht (`linenumber = row_number()`).
Im letzten Schritt zählen wir Kapitel. 
Zugegebenermaßen sieht `regex("^chapter [\\divxlc]"` furchteinflößend aus.
Der Befehl ist eine *regular expression* und sucht im Text nach dem Wort `chapter`.
`cumsum()` ist die kumulierte Summe der Kapitel.
Das alles speichern wir in `original_books`, mit `datatable()` haben wir die Möglichkeit, die Daten interaktiv zu begutachten.


```{webr}

original_books <- austen_books() %>%
  group_by(book) |> 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()

original_books |> head(n = 100) |> datatable()

```

# Tokenisierung

Im nächsten Schritt werden die Wörter *tokenisiert*.
*Tokeninsierung* bedeutet am Ende nichts anderes, als das ein langer Satz oder Text in seine Bestandteile zerlegt wird, um diesen dann für den Computer nutzbar und "verstehbar" zu machen.

Der Satz: *"Ich liebe Eis!"* würde in einer Tokenisierung in die Tokens **"Ich"**, **"liebe"**, **"Eis"** und **"!"** zerlegt werden.
Ein einzelener Satz kann hier noch von einem Menschen jederzeit zerlegt werden, bei einem umfangreichen Werk ist das aber nicht mehr wirklich möglich.

Mit dem Befehl `unnest_tokens()` wenden wir die *Tokeninsierung* auf alle Werke von Jane Austen an.
Es gibt unterschiedliche Arten von *token*, wir bleiben in diesem Beispiel bei *Wörtern*.
Die Ergebnisse können wir dann wieder in einer interaktiven Tabelle ansehen, diesesmal wählen wir $100$ zufällige Zeilen aus allen Datenreihen aus (`slice_sample(n=100)`).
Das die Tokenisierung funktioniert hat lässt sich am schnellsten an der Anzahl der Zeilen in den Datensätzen erkennen.
Im originales Datensatz sind insgesamt ca. $73.400$ Zeilen enthalten, während der tokenisierte Datensatz $>720.000$ Zeilen umfasst.
Jedes Token wird in eine neue Zeile geschrieben.

```{webr}

tidy_books <- original_books %>%
  unnest_tokens(word, text)

tidy_books |> slice_sample(n = 100) |> datatable()

```

## Stop Wörter

Es gibt einige Wörter die grammatikalisch zwar nötig sind, aber nicht viel zum Satzverständnis beitragen.
Diese Wörter werden *stop words* genannt.
Im Englischen sind dies extrem häufige Wörter wie z.B. "the", "of", "to".
Damit nicht jeder eigene *stop words* definieren muss, gibt es vorbereitete Lexika, die [öffentlich und frei zugänglich](http://snowball.tartarus.org/algorithms/english/stop.txt) sind [@lewis2004].
Aus unseren Daten können wir die *stop words* mit einem *anti join* entfernen.
Mit dem `nrow()` Befehl sehen wir schnell, dass sich die Anzahl der Datensätze drastisch auf $\approx 217.000$ verringert hat.

```{webr}

data(stop_words)

tidy_books <- tidy_books %>%
  anti_join(stop_words)

nrow(tidy_books)

```

# Wörter zählen

Die Autorin hat und hatte massive Einfluss auf die englische Literatur.
Die Frage ist: Wie hat sie das geschafft?
Mit den so vorbereiteten Daten beantworten läßt sich jetzt eine Frage recht schnell beantworten

*Welche Wörter hat Jane Austen am liebsten verwendet?*

Der Befehl `count()` riecht ja geradu passen, wir wollen diesen auf unseren Datensatz anwenden.

```{webr}

tidy_books %>%
  count(word, sort = TRUE) 

```

Aber eine Tabelle ist nicht immer die beste Art zur Darstellung von Daten.
Die Umgebung `R` verfügt über immense grafische Darstellungsbibliotheken.
Da dies aber kein `R` Kurs ist, sind die grafischen Darstellungen bereits vorbereitet.

```{webr}

tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(
    aes(
      y = word, 
      x = n)
    ) +
  geom_col(
    fill = "steelblue"
  )+
  scale_x_continuous(
    expand = c(0,0,0.05,0)
  )+
  labs(
    title = " Die meist-benutzten Wörter",
    x = "Häufigkeit der Wörter",
    y = ""
  )+
  theme_gray(base_size = 15)

```

Wir können die Ergebnisse auch in einer *wordcloud* darstellen.
Hierzunehmen wir maximal $100$ Wörter und richten diese zufällig in einer Art Kreis an.
Die Größe jedes Wortes entspricht hier der relativen Häufigkeit.

```{webr}

wrdcld <- tidy_books %>%
  anti_join(stop_words) %>%
  count(word)

par(bg = "white")
wordcloud(wrdcld$word,freq = wrdcld$n, max.words = 100)  

```


## Top 10 der Bücher

Wir wollen noch wissen inwiefern die Bücher sich voneinander unterscheiden und analysieren die Top 10 eines jeden Buches.

```{webr}
by_book <- tidy_books |> 
  group_by(book,word) |> 
  summarise(
    n_words = n()
  ) |> 
  top_n(10)
```

Die dazugehörige grafische Darstellung ist dann eben unten zu sehen.

```{webr}
#| fig-height: 7
#| 
by_book |> 
  ggplot(
    aes(
      x = n_words,
      y = reorder_within(word,n_words,book),
      fill = book
    )
  )+
  geom_col()+
  facet_wrap(
    ~book,
    scales = "free_y"
  )+
  scale_y_reordered()+
  scale_x_continuous(
    expand = c(0,0,0.05,0)
    )+
  scale_fill_brewer(
    palette = "Dark2"
  )+
  labs(
    x ="",
    y = "",
    fill = ""
  )+
  theme_bw()+
  theme(
    legend.position = "bottom"
    )

```

Am häufigsten in den Büchern kommen vor allem die Namen der Figuren vor!
Das ist nicht sonderlich überaschend, bestätigt aber auch den Schreibstil von Jane Austen.

## Top 20 der Bücher

Wir können die *Top* der Wörter auch auf die *Top 20* ändern.

```{webr}
by_book_top20 <- tidy_books |> 
  group_by(book,word) |> 
  summarise(
    n_words = n()
  ) |> 
  top_n(20)
```

Die dazugehörige grafische Darstellung ist dann eben unten zu sehen.

```{webr}
#| fig-height: 7

by_book_top20 |> 
  ggplot(
    aes(
      x = n_words,
      y = reorder_within(word,n_words,book),
      fill = book
    )
  )+
  geom_col()+
  facet_wrap(
    ~book,
    scales = "free_y"
  )+
  scale_y_reordered()+
  scale_x_continuous(
    expand = c(0,0,0.05,0)
    )+
  scale_fill_brewer(
    palette = "Dark2"
  )+
  labs(
    x ="",
    y = "",
    fill = ""
  )+
  theme_bw()+
  theme(
    legend.position = "bottom"
    )

```